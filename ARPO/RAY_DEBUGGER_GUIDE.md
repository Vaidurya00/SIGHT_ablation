# Ray Distributed Debugger ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨ Ray Distributed Debugger æ¥è°ƒè¯• ARPO é¡¹ç›®ã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

### 1. å®‰è£…å¿…è¦çš„è½¯ä»¶åŒ…

ç¡®ä¿å·²å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š

```bash
# ç¡®ä¿ Ray ç‰ˆæœ¬ >= 2.39
pip install "ray[default]>=2.39"

# å®‰è£… debugpy
pip install "debugpy>=1.8.0"
```

### 2. å®‰è£… VSCode æ‰©å±•

1. åœ¨ VSCode ä¸­å®‰è£… **Ray Distributed Debugger** æ‰©å±•
   - æ‰“å¼€ VSCode
   - è¿›å…¥ Extensions (Ctrl+Shift+X)
   - æœç´¢ "Ray Distributed Debugger"
   - ç‚¹å‡»å®‰è£…

## ğŸ”§ é…ç½®æ­¥éª¤

### æ­¥éª¤ 1: è®¾ç½®ç¯å¢ƒå˜é‡

åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ è°ƒè¯•ç›¸å…³çš„ç¯å¢ƒå˜é‡ã€‚ä¿®æ”¹ä½ çš„è®­ç»ƒè„šæœ¬ï¼ˆå¦‚ `ARPO_7B_Reasoning_1node.sh`ï¼‰ï¼Œåœ¨ç¯å¢ƒå˜é‡è®¾ç½®éƒ¨åˆ†æ·»åŠ ï¼š

```bash
# ============================ Environment Setup ============================
# Set basic environment variables
export PYTHONUNBUFFERED=1            
export HYDRA_FULL_ERROR=1           
export VLLM_ATTENTION_BACKEND=XFORMERS 
export VERL_LOGGING_LEVEL=DEBUG
export MKL_SERVICE_FORCE_INTEL=1    
export MKL_THREADING_LAYER=GNU       
export RAY_memory_usage_threshold=0.8  
export RAY_memory_monitor_refresh_ms=0 

# ========== Ray Distributed Debugger é…ç½® ==========
# å¯ç”¨ post-mortem è°ƒè¯•
export RAY_DEBUG_POST_MORTEM=1

# é‡è¦ï¼šç¡®ä¿ç§»é™¤æ—§çš„è°ƒè¯•æ ‡å¿—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
# ä¸è¦è®¾ç½® RAY_DEBUG=legacy
# ä¸è¦ä½¿ç”¨ --ray-debugger-external å‚æ•°
```

### æ­¥éª¤ 2: å¯åŠ¨ Ray é›†ç¾¤ï¼ˆå•æœºæ¨¡å¼ï¼‰

å¯¹äºå•æœºè®­ç»ƒï¼ŒRay ä¼šè‡ªåŠ¨åˆå§‹åŒ–ã€‚ä½†å¦‚æœä½ æƒ³æ‰‹åŠ¨æ§åˆ¶ï¼Œå¯ä»¥ï¼š

**é€‰é¡¹ A: ä½¿ç”¨è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆæ¨èç”¨äºå•æœºï¼‰**
- ç›´æ¥è¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œä»£ç ä¼šè‡ªåŠ¨è°ƒç”¨ `ray.init()`

**é€‰é¡¹ B: æ‰‹åŠ¨å¯åŠ¨ Ray é›†ç¾¤ï¼ˆç”¨äºå¤šèŠ‚ç‚¹æˆ–éœ€è¦ Dashboardï¼‰**
```bash
# å¯åŠ¨ head èŠ‚ç‚¹
ray start --head --dashboard-host=0.0.0.0 --port=6379

# æŸ¥çœ‹ Ray çŠ¶æ€
ray status

# è·å– Dashboard åœ°å€ï¼ˆé€šå¸¸æ˜¯ http://<head_node_ip>:8265ï¼‰
```

### æ­¥éª¤ 3: åœ¨ä»£ç ä¸­æ·»åŠ æ–­ç‚¹

åœ¨ä½ æƒ³è°ƒè¯•çš„ `@ray.remote` å‡½æ•°ä¸­æ·»åŠ  `breakpoint()`ã€‚ä¾‹å¦‚ï¼š

#### ç¤ºä¾‹ 1: åœ¨ TaskRunner ä¸­æ·»åŠ æ–­ç‚¹

ç¼–è¾‘ `/mnt/zhongwenlin/ARPO/ARPO/verl_arpo_entropy/verl/trainer/main_ppo.py`:

```python
@ray.remote(num_cpus=1)
class TaskRunner:
    def run(self, config):
        # åœ¨è¿™é‡Œæ·»åŠ æ–­ç‚¹
        breakpoint()  # è°ƒè¯•è®­ç»ƒå¼€å§‹æ—¶çš„é…ç½®
        
        from pprint import pprint
        from omegaconf import OmegaConf
        from verl.utils.fs import copy_to_local

        pprint(OmegaConf.to_container(config, resolve=True))
        OmegaConf.resolve(config)

        local_path = copy_to_local(config.actor_rollout_ref.model.path, use_shm=config.actor_rollout_ref.model.get("use_shm", False))
        
        # åœ¨è¿™é‡Œæ·»åŠ å¦ä¸€ä¸ªæ–­ç‚¹
        breakpoint()  # è°ƒè¯•æ¨¡å‹è·¯å¾„åŠ è½½
        
        # ... å…¶ä½™ä»£ç 
```

#### ç¤ºä¾‹ 2: åœ¨ Worker ç±»ä¸­æ·»åŠ æ–­ç‚¹

å¦‚æœä½ æƒ³è°ƒè¯•å…·ä½“çš„ worker é€»è¾‘ï¼Œéœ€è¦åœ¨ç›¸åº”çš„ worker ç±»ä¸­æ·»åŠ æ–­ç‚¹ã€‚ä¾‹å¦‚ï¼Œåœ¨ rollout worker ä¸­ï¼š

```python
# åœ¨ verl/workers/fsdp_workers.py æˆ–ç›¸å…³æ–‡ä»¶ä¸­
@ray.remote
class ActorRolloutRefWorker:
    def some_method(self, ...):
        breakpoint()  # åœ¨è¿™é‡Œæ·»åŠ æ–­ç‚¹
        # ... ä½ çš„ä»£ç 
```

### æ­¥éª¤ 4: åœ¨ VSCode ä¸­é…ç½® Ray é›†ç¾¤è¿æ¥

1. æ‰“å¼€ VSCode
2. ç‚¹å‡»å·¦ä¾§è¾¹æ çš„ **Ray Distributed Debugger** å›¾æ ‡
3. ç‚¹å‡» **"Add Cluster"** æˆ– **"+"** æŒ‰é’®
4. è¾“å…¥ Ray Dashboard åœ°å€ï¼š
   - å•æœºæ¨¡å¼ï¼š`http://localhost:8265`ï¼ˆå¦‚æœ Ray å·²å¯åŠ¨ï¼‰
   - å¤šèŠ‚ç‚¹æ¨¡å¼ï¼š`http://<head_node_ip>:8265`
   - å¦‚æœä½¿ç”¨è‡ªåŠ¨åˆå§‹åŒ–ï¼Œå¯èƒ½éœ€è¦å…ˆå¯åŠ¨ Ray Dashboard

### æ­¥éª¤ 5: è¿è¡Œè®­ç»ƒè„šæœ¬

ç›´æ¥è¿è¡Œä½ çš„è®­ç»ƒè„šæœ¬ï¼ˆ**ä¸è¦ä½¿ç”¨ launch.json**ï¼‰ï¼š

```bash
cd /mnt/zhongwenlin/ARPO/ARPO/scripts
conda activate arpo
bash ARPO_7B_Reasoning_1node.sh
```

æˆ–è€…ç›´æ¥è¿è¡Œ Python å‘½ä»¤ï¼š

```bash
python3 -m verl.trainer.main_ppo \
    --config-path=/mnt/zhongwenlin/ARPO/ARPO/scripts/config \
    --config-name=ppo_trainer.yaml \
    # ... å…¶ä»–å‚æ•°
```

### æ­¥éª¤ 6: é™„åŠ è°ƒè¯•å™¨

1. å½“ä»£ç æ‰§è¡Œåˆ° `breakpoint()` æ—¶ï¼Œç¨‹åºä¼šæš‚åœ
2. åœ¨ VSCode ä¸­ï¼Œç‚¹å‡» **Ray Distributed Debugger** ä¾§è¾¹æ å›¾æ ‡
3. ä½ ä¼šçœ‹åˆ°æ£€æµ‹åˆ°çš„æ–­ç‚¹ä¿¡æ¯
4. ç‚¹å‡» **"Attach"** æˆ–æ–­ç‚¹æ—è¾¹çš„è¿æ¥æŒ‰é’®æ¥é™„åŠ è°ƒè¯•å™¨
5. ç°åœ¨ä½ å¯ä»¥åœ¨ VSCode ä¸­ï¼š
   - æŸ¥çœ‹å˜é‡å€¼
   - å•æ­¥æ‰§è¡Œ
   - æŸ¥çœ‹è°ƒç”¨æ ˆ
   - ä¿®æ”¹å˜é‡å€¼ï¼ˆå¦‚æœæ”¯æŒï¼‰

### æ­¥éª¤ 7: å¤„ç†å¤šä¸ªæ–­ç‚¹

å¦‚æœä»£ç ä¸­æœ‰å¤šä¸ª `breakpoint()`ï¼š

1. å½“ç¬¬ä¸€ä¸ªæ–­ç‚¹è§¦å‘æ—¶ï¼Œé™„åŠ è°ƒè¯•å™¨
2. è°ƒè¯•å®Œæˆåï¼Œ**å…ˆæ–­å¼€å½“å‰è°ƒè¯•ä¼šè¯**
3. ç»§ç»­æ‰§è¡Œï¼Œå½“ä¸‹ä¸€ä¸ªæ–­ç‚¹è§¦å‘æ—¶ï¼Œå†æ¬¡é™„åŠ è°ƒè¯•å™¨

## ğŸ“ å®Œæ•´ç¤ºä¾‹è„šæœ¬

åˆ›å»ºä¸€ä¸ªæ”¯æŒè°ƒè¯•çš„è®­ç»ƒè„šæœ¬ `ARPO_7B_Reasoning_1node_debug.sh`:

```bash
#!/bin/bash

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PARENT_DIR="$(dirname "$SCRIPT_DIR")"
cd "$PARENT_DIR"
echo "Switched to parent directory: $PARENT_DIR"

# ============================ Environment Setup ============================
# Set basic environment variables
export PYTHONUNBUFFERED=1            
export HYDRA_FULL_ERROR=1           
export VLLM_ATTENTION_BACKEND=XFORMERS 
export VERL_LOGGING_LEVEL=DEBUG
export MKL_SERVICE_FORCE_INTEL=1    
export MKL_THREADING_LAYER=GNU       
export RAY_memory_usage_threshold=0.8  
export RAY_memory_monitor_refresh_ms=0 

# ========== Ray Distributed Debugger é…ç½® ==========
export RAY_DEBUG_POST_MORTEM=1

# Set Python path
export PYTHONPATH="/mnt/zhongwenlin/ARPO/ARPO"/verl_arpo_entropy:$PYTHONPATH

# ============================ Basic Configuration ============================
PROJECT_NAME="reasoning_tasks"
EXPERIMENT_NAME="ARPO_debug_test"
CONFIG_PATH="/mnt/zhongwenlin/ARPO/ARPO/scripts/config"
CONFIG_NAME="ppo_trainer.yaml"
NNODES=1                            
N_GPUS_PER_NODE=4                   

# ============================ Data Configuration ============================
PROMPT_KEY="prompt"
TRAIN_BATCH_SIZE=128
PPO_MINI_BATCH_SIZE=16
MAX_PROMPT_LENGTH=1536
MAX_RESPONSE_LENGTH=4096
TRAIN_FILES="/mnt/zhongwenlin/ARPO/ARPO/rl_datasets/hard_search_1k.parquet"
VALID_FILES="/mnt/zhongwenlin/ARPO/ARPO/rl_datasets/gaia_test.parquet"

# ============================ Model Configuration ============================
ACTOR_MODEL_PATH="/mnt/zhongwenlin/model/Qwen/Qwen2.5-3B-Instruct"

# ============================ Rollout Configuration ==========================
ROLLOUT_NAME="vllm"
ROLLOUT_MODE="sync_with_tool"
ROLLOUT_N=16
INITIAL_ROLLOUTS=8
BEAM_SIZE=2
BRANCH_PROBABILITY=0.5
Entropy_weight=0.2
SEARCH_CACHE_PATH="/mnt/zhongwenlin/ARPO/ARPO/search_cache/search_cache.json"

# ============================ Reward Model Configuration ==========================
REWARD_MANAGER="naive"
CUSTOM_REWARD_FUNCTION_PATH="/mnt/zhongwenlin/ARPO/ARPO/verl_arpo_entropy/verl/utils/reward_score/deep_research.py"
CUSTOM_REWARD_FUNCTION_NAME="compute_score"

# ============================ Training Configuration ============================
TOTAL_EPOCHS=2
SAVE_FREQ=5
TEST_FREQ=5

# ============================ Path Configuration ============================
SAVE_PATH="/mnt/zhongwenlin/ARPO/ARPO/checkpoint_save_dir/${EXPERIMENT_NAME}"
ROLLOUT_SAVE_PATH="${SAVE_PATH}/rollout"

# ============================ Preparation ============================
if [ ! -d "$SAVE_PATH" ]; then
    mkdir -p $SAVE_PATH
fi

if [ ! -d "$ROLLOUT_SAVE_PATH" ]; then
    mkdir -p $ROLLOUT_SAVE_PATH
fi

# ============================ Start Training (with Debug) ============================
echo "Starting training with Ray Distributed Debugger..."
echo "Make sure to attach the debugger in VSCode when breakpoints are hit!"

python3 -m verl.trainer.main_ppo \
    --config-path=$CONFIG_PATH \
    --config-name=$CONFIG_NAME \
    algorithm.adv_estimator=grpo \
    algorithm.kl_ctrl.kl_coef=0.0 \
    data.train_files=${TRAIN_FILES} \
    data.val_files=${VALID_FILES} \
    data.prompt_key=${PROMPT_KEY} \
    data.train_batch_size=${TRAIN_BATCH_SIZE} \
    data.max_prompt_length=${MAX_PROMPT_LENGTH} \
    data.max_response_length=${MAX_RESPONSE_LENGTH} \
    actor_rollout_ref.model.path=${ACTOR_MODEL_PATH} \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.ppo_mini_batch_size=${PPO_MINI_BATCH_SIZE} \
    actor_rollout_ref.actor.use_dynamic_bsz=True \
    actor_rollout_ref.actor.ppo_max_token_len_per_gpu=$((2*(MAX_PROMPT_LENGTH+MAX_RESPONSE_LENGTH))) \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.0 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=$((4*(MAX_PROMPT_LENGTH+MAX_RESPONSE_LENGTH))) \
    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
    actor_rollout_ref.rollout.name=${ROLLOUT_NAME} \
    actor_rollout_ref.rollout.mode=${ROLLOUT_MODE} \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.7 \
    actor_rollout_ref.rollout.n=${ROLLOUT_N} \
    actor_rollout_ref.rollout.initial_rollouts=${INITIAL_ROLLOUTS} \
    actor_rollout_ref.rollout.beam_size=${BEAM_SIZE} \
    actor_rollout_ref.rollout.branch_probability=${BRANCH_PROBABILITY} \
    actor_rollout_ref.rollout.entropy_weight=${Entropy_weight} \
    +actor_rollout_ref.rollout.tools.tool_instances.search.params.cache_file=${SEARCH_CACHE_PATH} \
    reward_model.reward_manager=${REWARD_MANAGER} \
    custom_reward_function.path=${CUSTOM_REWARD_FUNCTION_PATH} \
    custom_reward_function.name=${CUSTOM_REWARD_FUNCTION_NAME} \
    trainer.critic_warmup=0 \
    trainer.logger="[console, wandb]" \
    trainer.project_name=${PROJECT_NAME} \
    trainer.experiment_name=${EXPERIMENT_NAME} \
    trainer.n_gpus_per_node=${N_GPUS_PER_NODE} \
    trainer.nnodes=${NNODES} \
    trainer.save_freq=${SAVE_FREQ} \
    trainer.test_freq=${TEST_FREQ} \
    trainer.total_epochs=${TOTAL_EPOCHS} \
    trainer.default_local_dir=${SAVE_PATH} \
    trainer.val_before_train=False \
    trainer.rollout_data_dir=${ROLLOUT_SAVE_PATH} \
    hydra.run.dir=${SAVE_PATH}/outputs 2>&1 | tee ${SAVE_PATH}/run.log
```

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

1. **æ–­ç‚¹ä½ç½®é™åˆ¶**ï¼š
   - æ–­ç‚¹åªèƒ½åœ¨ `@ray.remote` è£…é¥°çš„å‡½æ•°å†…éƒ¨ä½¿ç”¨
   - ä¸èƒ½åœ¨ä¸»è¿›ç¨‹ï¼ˆé remote å‡½æ•°ï¼‰ä¸­ä½¿ç”¨ `breakpoint()`

2. **ç¯å¢ƒå˜é‡**ï¼š
   - å¿…é¡»è®¾ç½® `RAY_DEBUG_POST_MORTEM=1`
   - ä¸è¦ä½¿ç”¨æ—§çš„ `RAY_DEBUG=legacy` æ ‡å¿—
   - ä¸è¦ä½¿ç”¨ `--ray-debugger-external` å‚æ•°

3. **Ray ç‰ˆæœ¬**ï¼š
   - ç¡®ä¿ Ray ç‰ˆæœ¬ >= 2.39
   - å¦‚æœç‰ˆæœ¬è¿‡ä½ï¼Œè¯·å‡çº§ï¼š`pip install --upgrade "ray[default]>=2.39"`

4. **å¤šèŠ‚ç‚¹è°ƒè¯•**ï¼š
   - å¯¹äºå¤šèŠ‚ç‚¹è®­ç»ƒï¼Œéœ€è¦å…ˆæ‰‹åŠ¨å¯åŠ¨ Ray é›†ç¾¤
   - ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½èƒ½è®¿é—® Dashboard åœ°å€

5. **æ€§èƒ½å½±å“**ï¼š
   - è°ƒè¯•æ¨¡å¼å¯èƒ½ä¼šå½±å“è®­ç»ƒæ€§èƒ½
   - å»ºè®®åªåœ¨éœ€è¦è°ƒè¯•æ—¶å¯ç”¨

## ğŸ” è°ƒè¯•æŠ€å·§

1. **æŸ¥çœ‹ Ray Dashboard**ï¼š
   - è®¿é—® `http://localhost:8265` æŸ¥çœ‹ Ray é›†ç¾¤çŠ¶æ€
   - å¯ä»¥æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œæƒ…å†µå’Œèµ„æºä½¿ç”¨

2. **æ—¥å¿—æŸ¥çœ‹**ï¼š
   - è®­ç»ƒæ—¥å¿—ä¼šè¾“å‡ºåˆ° `${SAVE_PATH}/run.log`
   - å¯ä»¥åœ¨ç»ˆç«¯å®æ—¶æŸ¥çœ‹æ—¥å¿—

3. **é€æ­¥è°ƒè¯•**ï¼š
   - ä»ç®€å•çš„æ–­ç‚¹å¼€å§‹ï¼Œé€æ­¥æ·±å…¥
   - ä½¿ç”¨ VSCode çš„è°ƒè¯•æ§åˆ¶ï¼ˆç»§ç»­ã€å•æ­¥ã€æŸ¥çœ‹å˜é‡ç­‰ï¼‰

## ğŸ“š å‚è€ƒèµ„æº

- [Ray Distributed Debugger å®˜æ–¹æ–‡æ¡£](https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html)
- [VERL å¤šèŠ‚ç‚¹è®­ç»ƒæ–‡æ¡£](../verl_arpo_entropy/docs/start/multinode.rst)

## ğŸ› å¸¸è§é—®é¢˜

**Q: æ–­ç‚¹æ²¡æœ‰è§¦å‘ï¼Ÿ**
A: ç¡®ä¿ï¼š
- ä»£ç åœ¨ `@ray.remote` å‡½æ•°å†…
- è®¾ç½®äº† `RAY_DEBUG_POST_MORTEM=1`
- Ray ç‰ˆæœ¬ >= 2.39
- VSCode æ‰©å±•å·²æ­£ç¡®å®‰è£…

**Q: æ— æ³•è¿æ¥åˆ° Ray é›†ç¾¤ï¼Ÿ**
A: æ£€æŸ¥ï¼š
- Ray Dashboard æ˜¯å¦å¯è®¿é—®
- é˜²ç«å¢™è®¾ç½®
- ç½‘ç»œè¿æ¥

**Q: è°ƒè¯•å™¨é™„åŠ å¤±è´¥ï¼Ÿ**
A: ç¡®ä¿ï¼š
- `debugpy` å·²å®‰è£…
- Ray é›†ç¾¤æ­£å¸¸è¿è¡Œ
- æ²¡æœ‰ä½¿ç”¨æ—§çš„è°ƒè¯•æ ‡å¿—

